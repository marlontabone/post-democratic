{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM Text Generation using Animal Farm by George Orwell\n",
    "## AI Module IS53024B Coursework II.\n",
    "\n",
    "The Aim. What are we trying to build?\n",
    "\n",
    "Building on knowledge from Chapter 8 in the Deep Learning with Phyton book by Francois Chollet we will be generating text using a text file (.txt) containing Animal Farm as our source of reference. In order to build a language model we need lots of text data which is why I decided to use the book Animal Farm by English Novelist and Essayist George Orwell (also very well known for his 1984 book). \n",
    "\n",
    "Initially I had opted to use J.R.R. Tolkien Lord of the Rings books, however I found it difficult to distinguish in my results between Tolkein's use of old english and abstract newly created words by the language model under a high temperature. \n",
    "\n",
    "Given our decision to use Orwell's Animal Farm, this language model will learn patterns and a writing style specific and inspired by George Orwell. His choice of topics which include but are not limited to social injustice, opposition to totalitarianism and an explicit endorsement of democratic socialism will also come into play.\n",
    "\n",
    "The architecture we will be using to build this language model is bidirectional long-short-term-memory (L.S.T.M.) although this project could also be performed on 1D Convnet architecture or a stack LSTM just like the example in the Deep Learning with Python book. \n",
    "\n",
    "What is an long-short-term-memory? (Notes taken down from Deep Learning with Phyton for the exam)\n",
    "\n",
    "Developed by Hochreiter and Schmidhuber in 1997 long short term memory was the culminatorial product of their research on the vanishing gradient problem. In a nutshell the LSTM saves information for later, thus preventing older signals from gradually vanishing during processing. \n",
    "\n",
    "It is meant to to allow past information to be reinjected at a later time. This is how it fights the vanishing-gradient problem and by result fighting overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from dataset.txt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown url type: 'dataset.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-216ea2387289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m path = tensorflow.keras.utils.get_file( \n\u001b[1;32m     30\u001b[0m     \u001b[0;34m'dataset.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     origin = 'dataset.txt') \n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# reading text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Corpus length:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#printing length of text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI2020/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI2020/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI2020/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI2020/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;31m# accept a URL or a Request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfullurl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI2020/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url, data, headers, origin_req_host, unverifiable, method)\u001b[0m\n\u001b[1;32m    326\u001b[0m                  \u001b[0morigin_req_host\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munverifiable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                  method=None):\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munredirected_hdrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI2020/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mfull_url\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeleter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/AI2020/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_full_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown url type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplithost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown url type: 'dataset.txt'"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "#import spacy, and spacy English(en)model\n",
    "# spacy is used to work on text\n",
    "import spacy\n",
    "nlp = spacy.load('en') # nlp stands for natural language processing\n",
    "\n",
    "# importing necessary libraries to use throughout\n",
    "\n",
    "import tensorflow\n",
    "import codecs\n",
    "import collections\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "from six.moves import cPickle\n",
    "\n",
    "#importing layers,models and optimizers along with callbacks from Keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint \n",
    "\n",
    "# loading .txt file \n",
    "path = 'dataset.text'\n",
    "    \n",
    "text = open(path).read().lower() # reading text\n",
    "print('Corpus length:', len(text)) #printing length of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An understanding of the data at hand and a good preparation of data is a staple of any deep learing investigation. It is in fact the most important part. In the cells below I make use of the Spacy library to retrieve the words from Animal Farm using the library's tokenizer. I make an effort to minimise the number of potential words in my dictionary discarding of capital notation. Capital notation being only a matter of syntax makes it irrelevant to the task at hand as it does not deal with logic or sense behind word structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word list creating dunction which creates a list of words available in the .txt file\n",
    "def create_wordlist(doc):\n",
    "    wl = []\n",
    "    for word in doc:\n",
    "        if word.text not in (\"\\n\",\"\\n\\n\",'\\u2009','\\xa0'):\n",
    "            wl.append(word.text.lower())\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlist = []\n",
    "\n",
    "input_file = path\n",
    "#read data\n",
    "with codecs.open(input_file, \"r\") as f:\n",
    " data = f.read()\n",
    "        \n",
    "#create sentences\n",
    "doc = nlp(data) # nlp stands for natural language processing\n",
    "wl = create_wordlist(doc)\n",
    "wordlist = wordlist + wl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole of Animal Farm is now transformed into a single list of words, which means that I can now put together a dictionary of words which appear in the book, excluding any duplicates. In the cell below we will assign an index to each word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  4055\n"
     ]
    }
   ],
   "source": [
    "# print(wordlist)\n",
    "\n",
    "word_counts = collections.Counter(wordlist)\n",
    "# print(word_counts.most_common())\n",
    "\n",
    "# we can see that'the' appears 2219 time in the book Animal Farm, in comparison other words appear only once, these include 'spades' and 'dispelled'\n",
    "\n",
    "# Mapping from index to word : that's the vocabulary\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocab = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "words = [x[0] for x in word_counts.most_common()] # starts at the top from most common to least common at the bottom\n",
    "\n",
    "#size of the vocabulary\n",
    "vocab_size = len(words)\n",
    "print(\"vocab size: \", vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create the training data for our LSTM, we will opt to create two lists. Firstly, a sentences list which is meant to contain our sequences of words and secondly a list which will contain the next words to come after each of the sequence found in the sentences list. \n",
    "\n",
    "The way in which this will work, is that we take the 30th first words in the wordlist (dictionary) and the word with index 31 will be the the next word of this sequence and is thus added into the next words list as index 0. Jumping by a step of one we then continue this process and iterate till the last word in the word list. \n",
    "\n",
    "Going through this process we manage to yield 34949 sequences and therefore the same amount of next words for every sequence has also been predicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 34949\n",
      "['straw', ',', 'under', 'a', 'lantern', 'which', 'hung', 'from', 'a', 'beam', '.', 'he', 'was', 'twelve', 'years', 'old', 'and', 'had', 'lately', 'grown', 'rather', 'stout', ',', 'but', 'he', 'was', 'still', 'a', 'majesticlooking', 'pig']\n",
      "for\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted word sequences\n",
    "maxlen = 30\n",
    "\n",
    "# We sample a new sequence every `step` words\n",
    "step = 1\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up words)\n",
    "next_words = []\n",
    "\n",
    "for i in range(0, len(wordlist) - maxlen, step):\n",
    "    sentences.append(wordlist[i: i + maxlen])\n",
    "    next_words.append(wordlist[i + maxlen])\n",
    "    \n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# testing out our code to make sure we are getting the results we expect\n",
    "print(sentences[500])\n",
    "print(next_words[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we are getting somewhere we cannot expect an LSTM to digest a list of strings. Therefore we need to transform these lists into data which is easily digestable by the network we are proposing. One way to do this is to one-hot encode the lists into binary arrays. In doing so sequences of words will be reorganised into matrices made of boolean values in which true (1) values represent the index position of the word in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Next, one-hot encode the words into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, vocab_size), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), vocab_size), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence):\n",
    "        x[i, t, vocab[word]] = 1\n",
    "    y[i, vocab[next_words[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we build here is a bidirectional LSTM recurrent neural network followed by a drop out layer. We are expecting the network to provide us with a probability for each available word in the vocabulary to the next one after a given sentence. Therefore in regards the architecture of the model we opt to end with a dense layer the size of the vocabulary along with a softmax activation. With regards to callbacks we implement an Early Stopping function which feeds back every two epochs and stops training as soon as the validation loss stops improving after four epochs as stipulated by the patience parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Input, Bidirectional\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "\n",
    "# using a function to build model architecture in case we may use it again \n",
    "\n",
    "def bidir_LSTM(maxlen, vocab_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(Bidirectional(LSTM(rnn_size, activation=\"relu\"),input_shape=(maxlen, vocab_size)))\n",
    "    model.add(layers.Dropout(0.6)) # dropout layer \n",
    "    model.add(layers.Dense(vocab_size))\n",
    "    model.add(layers.Activation('softmax')) \n",
    "    \n",
    "    optimizer = tensorflow.keras.optimizers.RMSprop(lr=learning_rate)\n",
    "    callbacks=[EarlyStopping(patience=2, monitor='val_loss')]\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=[categorical_accuracy])\n",
    "    print(\"model built!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model built!\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_20 (Bidirectio (None, 256)               4284416   \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4055)              1042135   \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4055)              0         \n",
      "=================================================================\n",
      "Total params: 5,326,551\n",
      "Trainable params: 5,326,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_size = 128 # size of RNN\n",
    "learning_rate = 0.01 #learning rate\n",
    "\n",
    "Model_bidir = bidir_LSTM(maxlen, vocab_size)\n",
    "Model_bidir.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train on 31454 samples, validate on 3495 samples\n",
      "Epoch 1/50\n",
      "31454/31454 [==============================] - 728s 23ms/sample - loss: 420.3232 - categorical_accuracy: 0.0682 - val_loss: 6.2176 - val_categorical_accuracy: 0.0764\n",
      "Epoch 2/50\n",
      "31454/31454 [==============================] - 736s 23ms/sample - loss: 6.5216 - categorical_accuracy: 0.0837 - val_loss: 6.5826 - val_categorical_accuracy: 0.0987\n",
      "Epoch 3/50\n",
      "31454/31454 [==============================] - 698s 22ms/sample - loss: 6.7546 - categorical_accuracy: 0.0936 - val_loss: 6.6286 - val_categorical_accuracy: 0.1070\n",
      "Epoch 4/50\n",
      "31454/31454 [==============================] - 807s 26ms/sample - loss: 6.7792 - categorical_accuracy: 0.1027 - val_loss: 6.6122 - val_categorical_accuracy: 0.1302\n",
      "Epoch 5/50\n",
      "31454/31454 [==============================] - 688s 22ms/sample - loss: 6.7629 - categorical_accuracy: 0.1071 - val_loss: 6.7537 - val_categorical_accuracy: 0.1356\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32 # minibatch size\n",
    "num_epochs = 50 # number of epochs\n",
    "\n",
    "callbacks=[EarlyStopping(patience=4, monitor='val_loss'),\n",
    "          ModelCheckpoint(filepath='my_model_gen_sentences.{epoch:02d}-{val_loss:.2f}.hdf5',\\\n",
    "                           monitor='val_loss', verbose=0, mode='auto', period=2)]\n",
    "#fit the model\n",
    "\n",
    "history = Model_bidir.fit(x, y,\n",
    "                 batch_size=batch_size,\n",
    "                 shuffle=True,\n",
    "                 epochs=num_epochs,\n",
    "                 callbacks=callbacks,\n",
    "                 validation_split=0.1)\n",
    "\n",
    "#save the model\n",
    "Model_bidir.save('my_model_generate_sentences.h5')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems as though in training, the loss is stuck between 6.5 and 6.7 and does not seem to be improving much. It stalls in such a way that the Early Stopping callback decides to put an end to training at the 5th epoch. From what I am thinking this stall in loss improvement may be attributed to the learning rate which may be too high a value and also to the size of the RNN which may need to be slightly bigger to improve representational value. Never the less I will still try to generate text using this training. Taking on board points from previous courseworks, I have factored in the above results into tables to make for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Acc      Training_Loss\n",
      "--------------  ---------------\n",
      "Epoch 1 0.0682         420.323\n",
      "Epoch 2 0.0837           6.5216\n",
      "Epoch 3 0.0936           6.7546\n",
      "Epoch 4 0.1027           6.7792\n",
      "Epoch 5 0.1071           6.7629\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate([['Epoch 1 0.0682', 420.3232], ['Epoch 2 0.0837 ', 6.5216],['Epoch 3 0.0936', 6.7546], ['Epoch 4 0.1027', 6.7792], ['Epoch 5 0.1071 ',6.7629 ]], headers=['Training_Acc','Training_Loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val_Acc           Val_Loss\n",
      "--------------  ----------\n",
      "Epoch 1 0.0764      6.2176\n",
      "Epoch 2 0.0987      6.5826\n",
      "Epoch 3 0.1070      6.6286\n",
      "Epoch 4 0.1302      6.6122\n",
      "Epoch 5 0.1356      6.7537\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate([['Epoch 1 0.0764',  6.2176], ['Epoch 2 0.0987 ',6.5826],['Epoch 3 0.1070', 6.6286], ['Epoch 4 0.1302', 6.6122], ['Epoch 5 0.1356', 6.7537]], headers=['Val_Acc','Val_Loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the probability for a word to be drawn still depends directly on the probability of it being the next word due to our bidirectional LSTM Model. To tune this probability, we opt to introduce a “temperature” parameter to smooth or sharpen its value according to what we would like to see output in terms of word and paragraph structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ temperature: 0.2\n",
      ". . . . . . . . . . . . he was twelve years old and had lately grown rather stout but he was still a majesticlooking pig , and the farm , and the animals , it had been , but the animals was to the farm , but they had been the animals had been the farm , he had been the farm , and and the farm , they had not to , the animals , but had been the farm , he had been the farm , and napoleon had been , and they had been the in the farm , and and had been the the animals , and the pigs had been , and , and in the farm . he had\n",
      "------ temperature: 0.5\n",
      ". . . . . . . . . . . . he was twelve years old and had lately grown rather stout but he was still a majesticlooking pig , and the farm , and the animals , it had been , but the animals was to the farm , but they had been the animals had been the farm , he had been the farm , and and the farm , they had not to , the animals , but had been the farm , he had been the farm , and napoleon had been , and they had been the in the farm , and and had been the the animals , and the pigs had been , and , and in the farm . he had that there was had had been the which to be to the farm , but in the one were with all . by the that there was they , it was that now that but they on it had not been a and at his of the this , but and a . \" \" round snowball , and a a , \" had would be in the napoleon , but as snowball , \" snowball had been . he would be a them , and a in the animals , had been . he had napoleon 's , a\n",
      "------ temperature: 1.0\n",
      ". . . . . . . . . . . . he was twelve years old and had lately grown rather stout but he was still a majesticlooking pig , and the farm , and the animals , it had been , but the animals was to the farm , but they had been the animals had been the farm , he had been the farm , and and the farm , they had not to , the animals , but had been the farm , he had been the farm , and napoleon had been , and they had been the in the farm , and and had been the the animals , and the pigs had been , and , and in the farm . he had that there was had had been the which to be to the farm , but in the one were with all . by the that there was they , it was that now that but they on it had not been a and at his of the this , but and a . \" \" round snowball , and a a , \" had would be in the napoleon , but as snowball , \" snowball had been . he would be a them , and a in the animals , had been . he had napoleon 's , a his be . was were were this a a than had been of animals that he napoleon had he was would him . up , and had no into the . many as jones 's were to their a his it had had at in the he two them . all the from pasture - the farmhouse , you no an with the side , no in the animals for and they into , did not they them and for it for and two 's get time - was the who had for them the to and the at they\n",
      "------ temperature: 1.2\n",
      ". . . . . . . . . . . . he was twelve years old and had lately grown rather stout but he was still a majesticlooking pig , and the farm , and the animals , it had been , but the animals was to the farm , but they had been the animals had been the farm , he had been the farm , and and the farm , they had not to , the animals , but had been the farm , he had been the farm , and napoleon had been , and they had been the in the farm , and and had been the the animals , and the pigs had been , and , and in the farm . he had that there was had had been the which to be to the farm , but in the one were with all . by the that there was they , it was that now that but they on it had not been a and at his of the this , but and a . \" \" round snowball , and a a , \" had would be in the napoleon , but as snowball , \" snowball had been . he would be a them , and a in the animals , had been . he had napoleon 's , a his be . was were were this a a than had been of animals that he napoleon had he was would him . up , and had no into the . many as jones 's were to their a his it had had at in the he two them . all the from pasture - the farmhouse , you no an with the side , no in the animals for and they into , did not they them and for it for and two 's get time - was the who had for them the to and the at they ever work our and which one they went three in now after all , to it for the by napoleon and had which did not in their , who ! one called them that when the what . , human to 's had , battle as a them as \" day 's triumph of believed never that not . was be the at . for the them no napoleon . happened it very or day own of singing opened when is out to moment they had , a up a animal not it to a from beasts of however .\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "words_number = 100 # number of words to generate\n",
    "seed_sentences = 'he was twelve years old and had lately grown rather stout but he was still a majesticlooking pig'\n",
    "\n",
    "generated = ''\n",
    "sentence = []\n",
    "\n",
    "for i in range (maxlen):\n",
    "    sentence.append(\".\")\n",
    "\n",
    "seed = seed_sentences.split()\n",
    "\n",
    "for i in range(len(seed)):\n",
    "    sentence[maxlen-i-1]=seed[len(seed)-i-1]\n",
    "\n",
    "generated += ' '.join(sentence)\n",
    "\n",
    "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print('------ temperature:', temperature)\n",
    "    #the, we generate the text\n",
    "    for i in range(words_number):\n",
    "        #create the vector\n",
    "        x = np.zeros((1, maxlen, vocab_size))\n",
    "        for t, word in enumerate(sentence):\n",
    "            x[0, t, vocab[word]] = 1.\n",
    "\n",
    "        #calculate next word\n",
    "        preds = Model_bidir.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_word = vocabulary_inv[next_index]\n",
    "\n",
    "        #add the next word to the text\n",
    "        generated += \" \" + next_word\n",
    "        # shift the sentence by one, and and the next word at its end\n",
    "        sentence = sentence[1:] + [next_word]\n",
    "\n",
    "    #print the whole text\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Standard LSTM DLWP Example using Animal Farm by George Orwell\n",
    "\n",
    "Initially I decided to try and feed the Animal Farm text into the standard Deep Learning with Python Francois Chollet example in order to be able to compare and contrast with the results which are to be retrieved when we train the Biderectional LSTM model on the same text. Below are documented the generated results of the standard example, showing the first epoch and the final epoch at varying degrees of temperature. From these results we can observe a great improvement in the loss from 2.2382 to 0.8436. \n",
    "\n",
    "It is also very clear how varying degrees of tempreture affect the output generated, low tempretures tend to provide for very repetitive (as evident from the multiple and's in the first epoch at temp 0.2) and predictable text along with a realistic structure to the writing, the pattern of characters chosen therefore generate words which exists in the English languge. On the other hand higher tempretures provide for more flexible character generation, often ending up with new words which seem plausible making for an interesting read. A few examples of newly generated words from epoch 59 at temp 1.2 are \"pilutions\" and \"implarn\". At a higher temperature in the case of character level generation local structure starts to disintegrate and words often seem like a bunch of random character strings. \n",
    "\n",
    "It must be noted that this example involved character-level text generation where we were deciding what the next character should be. In the bidirectional example I decide to switch this to word-level text generation. \n",
    "\n",
    "epoch 1 Train on 56429 samples 56429/56429 [==============================] - 70s 1ms/sample - loss: 2.2382\n",
    "\n",
    "--- Generating with seed: \"rd in the drawing-room. it was also armounced that the gun w\"\n",
    "\n",
    "------ temperature: 0.2\n",
    "\n",
    "rd in the drawing-room. it was also armounced that the gun whe he was ster and were when the windmall and all and and and and and and and and and and and and and and and and and and and and the conss and and who and and and and and and the animals sure and and and the for the pigs the sting and whe windmall snow and and and and his seat of the for the windmall stor and and and were the seat the animals stor and and and be the windmell some and when the ani\n",
    "\n",
    "------ temperature: 1.2\n",
    "\n",
    "barabie, und men to the your dick ever and when then in but wariwnes sfout the pnitahgid, chle pirise ingly horded hork. in whil fos, whout sanf upy sore seal. but uxplanimals ands in siged the pigs on nit misse, af theife junainwapseld wos furven ofhibt upbed and the farm us. as eysyuons forsh hemware, re almible in whoul wag no comp is of mrike of tt aimadich af. whan engy a\" imminsmadsurims, hed animels dennlsher, bnglkion. themet of dyap,s. \"nrund tqee\n",
    "\n",
    "epoch 59 Train on 56429 samples 56429/56429 [==============================] - 60s 1ms/sample - loss: 0.8436\n",
    "\n",
    "--- Generating with seed: \"his sudden uprising of creatures whom they were used to thra\"\n",
    "\n",
    "------ temperature: 0.2\n",
    "\n",
    "his sudden uprising of creatures whom they were used to thrase which had been discoly. and sometimes said, the animals were appearing the four plang as though they were the commonous dogs and the pigs are discested to be work the salaces of the farm as somethie sains the animals were arrount to the other animals were arroling a terr will shals the pigs arough the same the windmill. he was also decoration of the farm a dicky, whee the stall and were all to\n",
    "\n",
    "------ temperature: 1.2\n",
    "\n",
    "le now \"and in seem a will implarn jones and ariom to them! thies were ebel for the paitcus for that benjamin was on the animals frightened a prens ?our. they raund, themselves, so not enouls cick and fredicite thein ew animals \"sauning back. all again, to seed food lags by the imby the animals work in thing broke querry. ducky, comradespr-ess turnes, a guis of instance-puddless. is seemed to pilutions. as took the since botters no one rapoleing, but but f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As directed by the coursework requirements apart from switching from character level text generation to word-level text generation I also decided to make use of a bidirectional LSTM, use callbacks EarlyStopping and ModuleCheckpoint and to also include a dropout layer in the model architecture to mitigate overfitting. These are all elements which were not included in the DLWP standard LSTM text example but these concepts were all discussed in Part II of DLWP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We finish off with a neural network and some scripts capable of generating text in line with Orwell's writing style although not quite! The raw result of the neural networks trained during both the standard DLWP example and the Bidirectional LSTM are not without any flaws however they are something...As alluded to previously increasing the size of the RNN and allowing longer training would most probably yield better results, as well as tuning the model so to limit a great amount of variance. Although indeed text is generated, often times it is difficult to find global meaning in it and to make sense of the paragraph as a whole which brings us to conclude that current deep learning is far behind something which is readable or which can be on-par with the writing of an author. The generated text seems to lack a narative unlike what an author would come to produce. I believe this is one of the main points why text generation fails to achieve global sense. \n",
    "\n",
    "How can we improve on this ?\n",
    "\n",
    "Perhaps the way to do this is to keep trying to improve the generation of sentences. This can be done by detecting patterns in the sequences of sentences in the whole of the novel and not simply looking for patterns in the sequences of words. This could come to reveal the context of a paragraph and would be able to be used to wisely select and formulate the  structure of the next sentence of the text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
