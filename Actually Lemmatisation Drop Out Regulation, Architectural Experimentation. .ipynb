{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marlontabone/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "\n",
    "#importing layers,models and optimizers along with callbacks from Keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "import codecs\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 69670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marlontabone/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading .txt file \n",
    "path = '1.txt'\n",
    "text = open(path).read().lower() # reading text\n",
    "print('Corpus length:', len(text)) #printing length of text\n",
    "#print(text)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "\n",
    "     clean_words= []\n",
    "\n",
    "     words = word_tokenize(text)\n",
    "    \n",
    "     for word in words:\n",
    "           word = word.strip(string.punctuation)\n",
    "           if len(word)>=1 and word.isdigit()==False and word not in (\"äì\",\"\\n\",\"Äô\",\"\\n\\n\",'\\u2009','\\xa0','n’t',',','.','\\n  \\n\\n  \\n',' \\n\\n','“','\\n\\n  \\n',':','-'):\n",
    "                    word = word.lower()\n",
    "                    clean_words.append(word)\n",
    "                        \n",
    "     return clean_words\n",
    "\n",
    "token=clean_doc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12426\n"
     ]
    }
   ],
   "source": [
    "wd = WordNetLemmatizer()\n",
    "newDict = []\n",
    "for w in token:\n",
    "        newDict.append(wd.lemmatize(w))\n",
    "vocab_size = len(newDict)\n",
    "#print(newDict)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len=30\n",
    "seq=[]\n",
    "for i in range(0,len(newDict)-sequence_len):\n",
    "    seq.append(newDict[i:i+sequence_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(seq)\n",
    "sequence = tokenizer.texts_to_sequences(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12396\n"
     ]
    }
   ],
   "source": [
    "#print(sequence[2])\n",
    "\n",
    "print(len(sequence))\n",
    "import numpy as np\n",
    "arr=np.array(sequence)\n",
    "\n",
    "X, Y = arr[:,:-1], arr[:,-1]\n",
    "Y=to_categorical(Y,num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 29, 150)           1863900   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 29, 1024)          4812800   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1024)              8392704   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12426)             12736650  \n",
      "=================================================================\n",
      "Total params: 28,855,654\n",
      "Trainable params: 28,855,654\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM,Embedding\n",
    "\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 150, input_length=seq_length))\n",
    "model.add(LSTM(1024, return_sequences=True))\n",
    "model.add(LSTM(1024)) \n",
    "model.add(layers.Dropout(0.2)) # dropout layer \n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "155/155 [==============================] - 559s 4s/step - loss: 7.0024 - accuracy: 0.0487\n",
      "Epoch 2/150\n",
      "155/155 [==============================] - 561s 4s/step - loss: 6.3133 - accuracy: 0.0569\n",
      "Epoch 3/150\n",
      "155/155 [==============================] - 587s 4s/step - loss: 6.1226 - accuracy: 0.0665\n",
      "Epoch 4/150\n",
      "155/155 [==============================] - 567s 4s/step - loss: 5.9788 - accuracy: 0.0725\n",
      "Epoch 5/150\n",
      "155/155 [==============================] - 770s 5s/step - loss: 5.8470 - accuracy: 0.0810\n",
      "Epoch 6/150\n",
      "155/155 [==============================] - 471s 3s/step - loss: 5.9363 - accuracy: 0.0770\n",
      "Epoch 7/150\n",
      "155/155 [==============================] - 501s 3s/step - loss: 5.8801 - accuracy: 0.0742\n",
      "Epoch 8/150\n",
      "155/155 [==============================] - 580s 4s/step - loss: 5.6999 - accuracy: 0.0813\n",
      "Epoch 9/150\n",
      "130/155 [========================>.....] - ETA: 1:32 - loss: 5.5697 - accuracy: 0.0865"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y, batch_size=80, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of extracted word sequences\n",
    "maxlen = 30\n",
    "\n",
    "# We sample a new sequence every `step` words\n",
    "step = 1\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up words)\n",
    "next_words = []\n",
    "\n",
    "for i in range(0, len(token) - maxlen, step):\n",
    "    sentences.append(token[i: i + maxlen])\n",
    "    next_words.append(token[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/marlontabone/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found yearsold worked dig assorted desk tall testified court morning prime minister whenever leader labour party gonzi left next glass herman public decide time guilty mepa electoral past micallef refused ferry came jeffrey pullicino orlando left bartolo seriously sant evarist labour general pullicino orlando even eye pullicino orlando challenging sant\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "tokenised_text = sentences[rand.randrange(len(sentences))]\n",
    "seed_text = TreebankWordDetokenizer().detokenize(tokenised_text)\n",
    "\n",
    "def generate_seq(model,tokenizer,seq_length,seed_text,n_words):\n",
    "\n",
    "     result = []\n",
    "     in_text =seed_text\n",
    "     for i in range(n_words):\n",
    "          encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "          encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "          yhat = model.predict_classes(encoded)\n",
    "          out_word = ''\n",
    "          for word,index in tokenizer.word_index.items():\n",
    "               if index==yhat:\n",
    "                  out_word = word\n",
    "                  break\n",
    "          in_text+=' ' + out_word\n",
    "          result.append(word)\n",
    "     return ' '.join(result)\n",
    "\n",
    "\n",
    "generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
    "#print(\"Seed:\"+seed_text)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
